{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nomen aus einem Tweet extrahieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation von pyspark und nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment konfigurieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.1.0,org.apache.spark:spark-sql-kafka-0-10_2.11:2.1.0,com.databricks:spark-avro_2.11:3.2.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import nltk\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.streaming import StreamingContext \n",
    "from pyspark.streaming.kafka import KafkaUtils \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download nltk packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweet Schema Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([                                                                                          \n",
    "    StructField(\"text\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSqlContextInstance(sparkContext):\n",
    "    if (\"sqlContextSingletonInstance\" not in globals()):\n",
    "        globals()[\"sqlContextSingletonInstance\"] = SQLContext(sparkContext)\n",
    "    return globals()[\"sqlContextSingletonInstance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    # Remove Retweet text\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # Remove Hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # Remove Hashtag from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_noun(text):\n",
    "    is_noun = lambda pos: pos[:2] == \"NN\" \n",
    "\n",
    "    tweet_tokenizer = nltk.tokenize.TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tokens = tweet_tokenizer.tokenize(text)\n",
    "    return [word for (word, pos) in nltk.pos_tag(tokens) if is_noun(pos)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Übung 3\n",
    "\n",
    "Implementation der \"processTweets\" Methode. Die Kommentare defineren welche aktion dort implementiert werden muss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTweets(time, rdd):\n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    try:\n",
    "        sql_context = getSqlContextInstance(rdd.context)\n",
    "\n",
    "        # Mittels des sql_context und des schemas das RDD in eine Dataframe umwandeln\n",
    "        \n",
    "\n",
    "        # Den Text aus dem Dataframe extrahieren, z.B mit select.\n",
    "        \n",
    "\n",
    "        # Die beiden Methoden clean_tweet und extract_noun als User defined function definieren.\n",
    "\n",
    "\n",
    "        # Ein Dataframe erstellen welches eine Column mit allen Nomen enthält, z.B mit withColumn.\n",
    "\n",
    "        \n",
    "        # Ein output Dataframe erstellen. Pro Nomen ein Eintrag.\n",
    "        \n",
    "\n",
    "        # Dataframe zusammenziehen mit collect() und alle Nomen ausgeben.\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error process tweet: \" + e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Übung 1\n",
    "\n",
    "Erstellen eines SparkContext mit dem Namen \"Twitter\" und eines StreamingContext der alle 10 Sekunden läuft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Übung 2\n",
    "\n",
    "Erstellen eines Kafka Direct Stream mittels den KafkaUtils. Das Topic in Kafka heisst \"Tweets\". Danach ein Map erstellen welches pro RDD die Methode \"processTweets\" aufruft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Übung 4\n",
    "\n",
    "Streaming Context starten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
